# Ultralytics ðŸš€ AGPL-3.0 License - https://ultralytics.com/license

[build-system]
# Define the build system requirements for the project
requires = ["setuptools>=70", "wheel"]
build-backend = "setuptools.build_meta"

[project]
# CLIP (Contrastive Language-Image Pre-Training) is a neural network trained on a variety of
# image-text pairs, capable of zero-shot prediction without task-specific training data
name = "clip"
version = "1.0"
description = "Contrastive Language-Image Pre-Training model by OpenAI"
authors = [
    {name = "OpenAI"}
]
requires-python = ">=3.7"
# Core dependencies needed for CLIP functionality
dependencies = [
    "ftfy",      # Text preprocessing library for fixing Unicode errors
    "regex",     # Enhanced regular expression library
    "tqdm",      # Progress bar library for tracking long-running operations
    "torch",     # PyTorch deep learning framework
    "torchvision",  # Computer vision library for PyTorch
]

[project.optional-dependencies]
# Additional dependencies for development and testing
dev = ["pytest"]  # Testing framework for Python

[tool.setuptools]
# Module and package configuration
py-modules = ["clip"]  # The main CLIP module
packages = {find = {exclude = ["tests*"]}}  # Auto-discover packages but exclude tests
include-package-data = true  # Include non-Python files specified in MANIFEST.in

[tool.codespell]
ignore-words-list = "whet,grey,writeable,finalY,RepResNet,Idenfy,WIT,Smoot,EHR,ROUGE,ALS,iTerm,Carmel,FPR,Hach,Calle,ore,COO,MOT,crate,nd,ned,strack,dota,ane,segway,fo,gool,winn,commend,bloc,nam,afterall,skelton,goin"
skip = "Interacting_with_CLIP.ipynb,clip*.txt,clip*.json,*.pt,*.pth,*.torchscript,*.onnx,*.tflite,*.pb,*.bin,*.param,*.mlmodel,*.engine,*.npy,*.data*,*.csv,*pnnx*,*venv*,*translat*,*lock*,__pycache__*,*.ico,*.jpg,*.png,*.webp,*.avif,*.mp4,*.mov,/runs,/.git,./docs/??/*.md,./docs/mkdocs_??.yml"
